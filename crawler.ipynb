{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "crawler_startercode.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uPnseHUztBY"
      },
      "source": [
        "# Crawler\n",
        "\n",
        "This notebook contains started code structure for creating a crawler on single machine\n",
        "\n",
        "**Author:** Noshaba Nasir\n",
        "\n",
        "**Date:** 26/3/2021\n",
        "\n",
        "**Updated by:** Muhammad Usman Zahoor 17L-4102\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjjcAN_LztBZ"
      },
      "source": [
        "import os \n",
        "import random\n",
        "from urllib.parse import urlparse\n",
        "import requests\n",
        "import heapq\n",
        "import urllib.request\n",
        "from bs4 import BeautifulSoup\n",
        "import threading\n",
        "# Add any library to be imported here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecv12x2OztBa"
      },
      "source": [
        "# Crawler Parameters\n",
        "BACKQUEUES= 3\n",
        "THREADS= BACKQUEUES*3\n",
        "FRONTQUEUES= 5\n",
        "WAITTIME= 15 ; # wait 15 seconds before fetching URLS from \n",
        "visited = []\n",
        "\n",
        "# Add any other global parameters here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnFpncHRztBa"
      },
      "source": [
        "# FRONTIER\n",
        "\n",
        "Frontier should use the Mercator frontier design as discussed in lecture.\n",
        "\n",
        "Preferably it should be a class and should have the given functions.\n",
        "\n",
        "*prioritizer* function is a stub right now, it will return a random number  between 1 to f for given URL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Xo8OHVSdztBb"
      },
      "source": [
        "class frontier:\n",
        "# add the code for frontier here\n",
        "# should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
        "  def __init__(self):\n",
        "        '''\n",
        "        Initializer\n",
        "        '''\n",
        "        self.back_queue_time_stamp=[0 for i in range(BACKQUEUES)]\n",
        "        self.back_queues = [[] for i in range(BACKQUEUES)]\n",
        "        self.front_queues=[[] for i in range(FRONTQUEUES)]\n",
        "        self.seed_urls = [\"https://docs.oracle.com/en/\",\"https://www.oracle.com/corporate/\",\n",
        "                        \"https://en.wikipedia.org/wiki/Machine_learning\",\"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\",\n",
        "                        \"https://docs.oracle.com/middleware/jet210/jet/index.html\",\"https://en.wikipedia.org/w/api.php\",\"https://en.wikipedia.org/api/\",\n",
        "                        \"https://en.wikipedia.org/wiki/Weka_(machine_learning)\"]\n",
        "        for i in self.seed_urls:\n",
        "          self.addURLs(i)\n",
        "                      \n",
        "  def get_URL(self):\n",
        "    if len(self.back_queues)>0:\n",
        "      i = BACKQUEUES-1\n",
        "      while i >=0 :\n",
        "        if len(self.back_queues[i]) > 0:\n",
        "          self.back_queue_time_stamp[i]+=WAITTIME \n",
        "          return self.back_queues[i].pop(0)\n",
        "        i-=1\n",
        "    return self.getURL_from_front()\n",
        "\n",
        "  #adding to front queue\n",
        "  def addURLs(self,url):\n",
        "    priority = prioritizer(url, FRONTQUEUES-1)\n",
        "    self.front_queues[priority-1].append(url)\n",
        "\n",
        "    \n",
        "  def check_empty_backqueue(self):\n",
        "    for i in range(BACKQUEUES):\n",
        "      if len(self.back_queues[i])==0:\n",
        "        return i\n",
        "    return -1\n",
        "\n",
        "  def getURL_from_front(self):\n",
        "    i = FRONTQUEUES-1\n",
        "    while i >=0 :\n",
        "      if len(self.front_queues[i]) > 0:\n",
        "        return self.front_queues[i].pop(0)\n",
        "      i-=1\n",
        "\n",
        "  def add_to_backqueue(self):\n",
        "    flag = True\n",
        "    while flag == True:\n",
        "      url = self.getURL_from_front()\n",
        "      addNew = True\n",
        "      empty_queue = self.check_empty_backqueue();\n",
        "      if empty_queue == -1:\n",
        "        flag = False\n",
        "      else:\n",
        "        domain = urlparse(url).netloc\n",
        "        for i in range(BACKQUEUES):\n",
        "          if domain == urlparse(self.back_queues[i]).netloc:\n",
        "            self.back_queues[i].append(url)\n",
        "            addNew = False\n",
        "\n",
        "        if addNew == True:\n",
        "          self.back_queues[empty_queue].append(url)\n",
        "  # add more functions here\n",
        "\n",
        "\n",
        "\n",
        "def prioritizer(URL,f):\n",
        "    \"\"\"\n",
        "    Take URL and returns priority from 1 to F\n",
        "    Right now it like a stub function. \n",
        "    It will return a random number from 1 to f for given inputs. \n",
        "    \"\"\"\n",
        "    return random.randint(1,f)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGtYxtYNztBb"
      },
      "source": [
        "# FILTER URLS\n",
        "\n",
        "Filter the URLS that are in robots.txt files of server and the have been already processed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thwj3HiEztBb"
      },
      "source": [
        "# write code for filtering urls here \n",
        "\n",
        "def filter_url(url):\n",
        "  domain = urlparse(url).netloc\n",
        "  if domain is not None:\n",
        "    robotPath = \"http://\"+domain+\"/robots.txt\"\n",
        "    robot = requests.get(robotPath)\n",
        "    my_list = list(robot.text.split('\\n'))\n",
        "    for ur in my_list:\n",
        "      if (ur.split(':')[0] == \"Disallow\"):\n",
        "        nUrl =  \"http://\" + domain + (ur.split(':')[1]).strip()\n",
        "        if nUrl == url:\n",
        "          return False\n",
        "    return True\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcQbsgeEztBb"
      },
      "source": [
        "## ---------------------Write any other codes/data require to run the crawler above this cell-----------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfvEBe5lztBc"
      },
      "source": [
        "# Run Crawler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7isLm75ztBc"
      },
      "source": [
        "def get_links(doc,url):\n",
        "  beauty = BeautifulSoup(doc, 'lxml')\n",
        "  links_to_return = set()\n",
        "  links = beauty.findAll('a',href=True)\n",
        "  for link in links:\n",
        "    address = link.attrs['href'];\n",
        "    if 'http' in address:\n",
        "      links_to_return.add(address)\n",
        "  return links_to_return\n",
        "  \n",
        "\n",
        "def get_data_from_url(url):\n",
        "  with urllib.request.urlopen(url) as response:\n",
        "    data = response.read()\n",
        "    return data\n",
        "\n",
        "# Theard task\n",
        "def crawler_thread_task(frontier_,count):\n",
        "  url = frontier_.get_URL()\n",
        "  while url is  None:\n",
        "    url = frontier_.get_URL()\n",
        "  if url is not None:\n",
        "    if url[0] != \"#\": \n",
        "      print(\"Thread \"+  str(count)+ \" Visiting \" + url+\"\\n\")\n",
        "    links = set()\n",
        "    try:\n",
        "      links = get_links(get_data_from_url(url),url)\n",
        "    except:\n",
        "      print(\"Error in \"+ \"Thread \"+ str(count)+\"\\n\")\n",
        "    visited.append(url)\n",
        "    for link in links:\n",
        "      if link not in visited:\n",
        "        frontier_.addURLs(link);\n",
        "    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srN5O9alztBc"
      },
      "source": [
        "\n",
        "links = get_links(get_data_from_url(\"https://en.wikipedia.org/wiki/Machine_learning\"),\"https://en.wikipedia.org/wiki/Machine_learning\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj-Cz9VYztBc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d2b42fa-a176-4c64-a399-bab0bd77086c"
      },
      "source": [
        "# start the threads\n",
        "frontier_ = frontier()\n",
        "threads=[]\n",
        "count = 0\n",
        "while count<100:\n",
        "  \n",
        "  try:\n",
        "    threads.clear()\n",
        "    for j in range(THREADS):\n",
        "      t1 = threading.Thread(target=crawler_thread_task, args=(frontier_,j))\n",
        "      t1.start()\n",
        "      threads.append(t1) \n",
        "      count+=1\n",
        "    for j in threads:\n",
        "      j.join()\n",
        "  except:\n",
        "    print(\"Error\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thread 0 Visiting https://docs.oracle.com/middleware/jet210/jet/index.html\n",
            "\n",
            "Thread 1 Visiting https://www.oracle.com/corporate/\n",
            "\n",
            "Thread 2 Visiting https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\n",
            "Thread 3 Visiting https://en.wikipedia.org/w/api.php\n",
            "\n",
            "Thread 4 Visiting https://en.wikipedia.org/wiki/Weka_(machine_learning)\n",
            "\n",
            "Thread 5 Visiting https://en.wikipedia.org/wiki/Machine_learning\n",
            "\n",
            "Thread 6 Visiting https://docs.oracle.com/en/\n",
            "Thread 7 Visiting https://en.wikipedia.org/api/\n",
            "\n",
            "\n",
            "\n",
            "Thread 8 Visiting https://foundation.wikimedia.org/wiki/Developer_app_guidelines\n",
            "\n",
            "Thread 0 Visiting http://ricardo.ecn.wfu.edu/pub/gretl/svm/gretl-svm.pdf\n",
            "\n",
            "Thread 1 Visiting http://www.causality.inf.ethz.ch/activelearning.php\n",
            "Thread 2 Visiting http://sds.podval.org\n",
            "\n",
            "\n",
            "Thread 3 Visiting http://www.csie.ntu.edu.tw/~cjlin/libsvm/COPYRIGHT\n",
            "\n",
            "Thread 4 Visiting http://www.python.org/download/\n",
            "Thread 5 Visiting https://github.com/nicolaspanel/node-svm/tree/master\n",
            "\n",
            "\n",
            "Thread 6 Visiting http://quotenil.com\n",
            "\n",
            "Thread 7 Visiting http://www.davidsoergel.com\n",
            "\n",
            "Thread 8 Visiting http://pecl.php.net/package/svm\n",
            "\n",
            "Error in Thread 2\n",
            "\n",
            "Thread 0 Visiting https://bitbucket.org/ogu/libsvm-ocaml/\n",
            "\n",
            "Thread 1 Visiting http://cran.r-project.org/web/packages/e1071/index.html\n",
            "\n",
            "Thread 2 Visiting http://www.youtube.com/watch?v=gePWtNAQcK8\n",
            "Thread 3 Visiting https://www.linkedin.com/company/oracle/\n",
            "\n",
            "\n",
            "Thread 4 Visiting https://academy.oracle.com/en/oa-web-overview.html\n",
            "\n",
            "Thread 5 Visiting https://nl.wikipedia.org/wiki/Weka_(software)\n",
            "Thread 6 Visiting https://www.wikidata.org/wiki/Special:EntityPage/Q115494\n",
            "\n",
            "\n",
            "Thread 7 Visiting https://eu.wikipedia.org/wiki/Weka_(ikasketa_automatikoa)\n",
            "Thread 8 Visiting https://zh.wikipedia.org/wiki/Weka\n",
            "\n",
            "\n",
            "Error in Thread 3\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiAA4xcIztBd"
      },
      "source": [
        "## ------------------------------------------------------End of Notebook---------------------------------------------------"
      ]
    }
  ]
}